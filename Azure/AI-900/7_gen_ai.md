- Generative AI is a branch of AI to generate new content; 
  - often natural language dialogs, 
  - but also images, video, code, and other formats.
- Generative AI models encapsulate semantic relationships between language elements 
  - i.e. models "know" how words relate to one another
  - and that's what enables them to generate a meaningful sequence of text.
- large language models (LLMs) and small language models (SLMs)
  - LLMs are very powerful and generalize well, but can be more costly to train and use. 
  - SLMs are more focused on specific topic areas, and usually cost less.
- uses of generative AI:
  - chatbots and AI agents to assist human users. 
  - create new documents or other content
  - Automate translation of text between languages.
  - Summarize or explain complex documents.
  - Comparing multiple text sources for semantic similarity.
  - Generating new natural language.
  - Determining sentiment or otherwise classifying natural language text.
-  can combine computer vision and language models to create 
  - a multi-modal model that combines computer vision and generative AI capabilities.
- Transformers
  - Transformer models
    - trained with large volumes of text(natural language text) sourced from the internet or other public sources of text.
    - enabling them to represent the semantic relationships between words and
    - use those relationships to determine probable sequences of text that make sense.
  - Transformer model architecture
    - encoder block
      - creates semantic representations of the training vocabulary.
    - decoder block
      - generates new language sequences.
  - Process
    - train with a large volume of natural language text
    - sequences of text are broken down into tokens
      - encoder block processes these token sequences using a technique called attention 
        - to determine relationships between tokens
    - output from the encoder is a collection of vectors (multivalued numeric arrays)
      - each element of the vector represents a semantic attribute of the tokens. 
      - These vectors are referred to as embeddings.
    - decoder block works on a new sequence of text tokens
      - uses the embeddings generated by the encoder to generate an appropriate natural language output.
    - e.g.
      - input sequence like "When my dog was"
      - predict an appropriate completion of the sentence, such as "a puppy".
  - Models
    - Bidirectional Encoder Representations from Transformers (BERT) model developed by Google
      - to support their search engine uses only the encoder block,
    - Generative Pretrained Transformer (GPT) model developed by OpenAI
      - uses only the decoder block.
- Tokenization
  - decompose the training text into tokens
  - Embeddings
    - contextual vectors
    - multivalued numeric representations of information
    - Vectors represent lines in multidimensional space
    - describing direction and distance along multiple axes
      - called amplitude and magnitude
    - elements in an embedding vector for a token as representing steps along a path in multidimensional space
    - cosine similarity
      - used to determine if two vectors have similar directions
      - regardless of distance
      - represent semantically linked words
    - Attention
      - attention layers
        - examine a sequence of text tokens and
        - try to quantify the strength of the relationships between them.
        - self-attention involves considering how other tokens around one particular token influence that token's meaning.
        - In encoder, each token is carefully examined in context
        - vector values are based on the relationship between the token
        - same word might have multiple embeddings
          - the bark of a tree
          - I heard a dog bark
        - decoder block
          - attention layers are used to predict the next token in a sequence
          - consider which of the tokens are the most influential when considering what the next token should be.
      - multi-head attention
        - uses different elements of the embeddings to calculate multiple attention scores.
        - building the output one token at a time.
    - Steps:
    - A sequence of token embeddings is fed into the attention layer. Each token is represented as a vector of numeric values.
    - The goal in a decoder is to predict the next token in the sequence, which will also be a vector that aligns to an embedding in the modelâ€™s vocabulary.
    - The attention layer evaluates the sequence so far and assigns weights to each token to represent their relative influence on the next token.
    - The weights can be used to compute a new vector for the next token with an attention score. Multi-head attention uses different elements in the embeddings to calculate multiple alternative tokens.
    - A fully connected neural network uses the scores in the calculated vectors to predict the most probable token from the entire vocabulary.
    - The predicted output is appended to the sequence so far, which is used as the input for the next iteration.
