- Machine Learning(GeeksforGeeks)
  - branch of Artificial Intelligence that focuses on developing models and algorithms
  - Problem Statement
    - Project Objectives
    - Output Expectations
    - Success Criteria
  - Data Collection
    - collection of datasets used as raw data to train model
  - Data Preprocessing
    - Data Cleaning
      - resolve issues like:
        - Missing Values
        - Duplicate Data
        - Outliers
          - Outlier Detection Methods
            - Z-Score Method
            - IQR (Interquartile Range) Method
              - range between the first quartile (Q1) and the third quartile (Q3) in a dataset.
              - IQR = Q3(75th percentile) - Q1(25th percentile)
              - 
            - Visualization Techniques
              - Box Plots
              - Scatter Plots
        - Inconsistent Data
        - Irrelevant Data
    - Sampling Data
    - Dimensionality Reduction
      - Tools:
        - OpenRefine
        - Trifacta Wrangler
        - TIBCO Clarity
        - Cloudingo
        - IBM InfoSphere QualityStage
    - Data Preprocessing Techniques
      - Normalization(Min-Max Scaling)
        - Rescales features between 0 and 1.
        - useful when the data does not follow a Gaussian distribution.
        - sensitive to outliers, as extreme values can significantly affect the scaling.
        - Good for algorithms like k-NN and neural networks.
      - Standardization
      - Encoding Categorical Variables
    - Data Transformation
  - Data Exploration: uncover insights, patterns, and relationships within the data
    - Descriptive Statistics
    - Data Visualization
    - Correlation Analysis
    - EDA: Exploratory Data Analysis
      - visualize data 
      - understand main features,
      - find patterns and 
      - discover how parts of data are connected.
      - Types of EDA
        - Univariate Analysis
          - analyze single variable and
          - understand its distribution and characteristics.
          - e.g. histograms are used to show data distribution, 
            - box plots to detect outliers and understand data spread and 
            - bar charts for categorical data.
        - Bivariate Analysis
          - explore relationship between two variables
          - identify correlations or associations.
          - Key techniques
            - Scatter Plots
            - Correlation Coefficients
              - Pearson’s Correlation for linear relationships between continuous variables
              - Spearman’s Rank Correlation
            - Cross-tabulations or Contingency Tables
            - Heatmaps
            - Line Graphs
            - Covariance Analysis
              - measures how two variables change together
        - Multivariate Analysis
          - examine interactions among three or more variables
          - uncover complex relationships and patterns.
          - techniques
            - Pair Plots
              - relationship between multiple variables at once
            - 3D Scatter Plots
            - Principal Component Analysis (PCA)
              - linear technique that reduces the dimensionality of data by 
              - transforming the original features into a smaller set of uncorrelated features 
              - called principal components.
            - Cluster Analysis
            - Multivariate Regression
            - Spatial Analysis
              - used for geographical data by using maps
              - and spatial plotting
            - Time Series Analysis
              - analyze data points collected or recorded at specific time intervals.
              - Techniques
                - Line Plots
                - Seasonal Decomposition
                - Autocorrelation Analysis
                - Moving Averages
                - ARIMA Models
                  - ARIMA (AutoRegressive Integrated Moving Average)
                  - used for forecasting and understanding time series data.
    - Time Series Analysis
      - Concepts
        - Trend
          - direction in which a time series is moving
        - Seasonality
          - repetitive patterns or cycles that occur at regular intervals
        - Moving Averages
          - smooth out short-term fluctuations and highlight longer-term trends
        - Noise
          - irregular and unpredictable components
        - Differencing
          - make the difference in values of a specified interval.
        - Stationarity time series
          - statistical properties such as mean, variance and autocorrelation remain constant over time.
        - Order of Differencing
          - number of times the time series data needs to be differenced to achieve stationarity.
        - Autocorrelation Function (ACF)
          - quantify the degree of similarity between a time series and a lagged version of itself.
        - Resampling
          - changing the frequency of the data observations.
        - Cyclic Patterns
        - Irregular Components
        - Decomposition Methods
          - Additive Decomposition
          - Multiplicative Decomposition
      - Types
        - Continuous Time Series
          - Data recorded at regular intervals with a continuous range of values 
          - like temperature, stock prices, Sensor Data, etc.
        - Discrete Time Series
          - recorded at specific time points like counts of events, categorical statuses, etc.
  - Feature Engineering: selecting only relevant variables(features)
    - create new feature or transform existing features
    - Feature Creation
      - generating new features from domain knowledge or by observing patterns in the data.
      - Interaction Features
      - Polynomial Features
    - Feature Transformation
      - Normalization & Scaling
      - Encoding
      - Mathematical transformations
        - e.g. logarithmic transformations for skewed data
    - Feature Selection: choosing only the most useful input features
      - Filter Methods
        - evaluate each feature independently with target variable
        - Based on statistical measures like correlation.
        - Common Filter Techniques
          - Information Gain: 
            - Measures reduction in entropy when a feature is used.
          - Chi-square test: 
            - Checks the relationship between categorical features.
          - Fisher’s Score: 
            - Ranks features based on class separability.
          - Pearson’s Correlation Coefficient: 
            - Measures linear relationship between two continuous variables.
          - Variance Threshold: 
            - Removes features with very low variance.
          - Mean Absolute Difference: 
            - Similar to variance threshold but uses absolute differences.
          - Dispersion ratio: 
            - Ratio of arithmetic mean to geometric mean; 
            - higher values indicate useful features.
      - Wrapper Methods
        - greedy algorithms that train algorithm
        - use different combination of features and compute relation between these subset features and target variable
        - Common Wrapper Techniques
          - Forward Selection
            - start with no features
            - add features one by one based on model performance improvement.
          - Backward Elimination
            - start with all features
            - remove least significant features iteratively.
          - Recursive Feature Elimination (RFE)
            - recursively removes least important features
        - Select based on model performance.
      - Embedded Methods
        - Feature selection integrated within model training.
        - Combine the benefits of both filter and wrapper methods
        - Common Embedded Techniques
          - Lasso Regression (L1 Regularization)
            - Keeps only features with non-zero coefficients.
            - adds penalty equal to the absolute value of the magnitude of coefficients to prevent overfitting.
            - can shrink some coefficients to zero, effectively performing feature selection.
            - J(λ) = Loss + λ * Σ|coefficients|
          - Ridge Regression (L2 Regularization)
            - adds penalty equal to the square of the magnitude of coefficients.
            - penalize large coefficient in linear regression equation
            - does not perform feature selection but helps in reducing multicollinearity.
            - J(λ) = Loss + λ * Σ(coefficients)²
          - Elastic Net Regression
            - combines L1 and L2 regularization
            - balances between feature selection and coefficient shrinkage.
            - J(λ1, λ2) = Loss + λ1 * Σ|coefficients| + λ2 * Σ(coefficients)²
          - Decision Trees and Random Forests
            - provide feature importance scores based on how much each feature contributes to 
              - reducing impurity.
          - Gradient Boosting Machines (GBM)
            - similar to Random Forests
            - can be used to assess feature importance.
            - pick features that reduce prediction error the most.
    - Feature Scaling: 
      - standardize the range of independent variables or features of data.
      - Scaling Techniques
        - Absolute Maximum Scaling
          - rescales each feature by dividing all values by the maximum absolute value of that feature
          - feature values fall within the range of -1 to 1
          - highly sensitive to outliers
        - Min-Max Scaling(Normalization)
          - transforms by subtracting the minimum value and 
          - dividing by the difference between the maximum and minimum values.
          - rescales the values to a specified range, typically between 0 and 1.
          - preserving the original distribution shape 
          - but is still affected by outliers
        - Vector Normalization
          - scales each data sample (row) such that its vector length (Euclidean norm) is 1
          - focuses on the direction of data points rather than magnitude
        - Standardization (Z-score Normalization)
          - transforms the values to have a mean of 0 and a standard deviation of 1.
          - centers the data around the mean and scales it based on the standard deviation.
          - useful when the data follows a Gaussian distribution.
          - less sensitive to outliers compared to Min-Max Scaling.
          - preserves the shape of the original distribution of the data.
          - subtracting the mean and scales them by dividing by the standard deviation
          - normal distribution often benefits models like linear regression, 
            - logistic regression and neural networks by improving convergence speed and stability.
          - Z = (X - μ) / σ
            - X = Data
              μ = Mean value of X
              σ = Standard deviation of X
        - Robust Scaling
          - uses the median and interquartile range (IQR) instead of the mean and standard deviation
          - transformation robust to outliers and skewed distributions.
          - suitable when the dataset contains extreme values or noise.
          - Reduces influence of outliers by centering on median
    - Feature Extraction
      - transforming raw data into a simplified and informative set of features or attributes.
      - reduces data complexity and highlights the most relevant information
      - Techniques
        - Statistical Methods
          - Mean: average value of a dataset.
          - Median: middle value when data is ordered.
          - Mode: most frequently occurring value.
          - Standard Deviation: measure of data dispersion around the mean.
          - corelation and covariance: measure relationships between variables.
          - Regression Analysis: understand relationships between dependent and independent variables.
          - Variance: measure of data spread.
          - Skewness: measure of data asymmetry.
          - Kurtosis: measure of data "tailedness".
        - Dimensionality Reduction Techniques
          - Principal Component Analysis (PCA)
            - select variables that account for most of the data’s variation
            - reduces dimensionality while preserving important information
          - Linear Discriminant Analysis (LDA)
            - find the best combination of features to separate different classes
          - t-Distributed Stochastic Neighbor Embedding (t-SNE)
            - reduce high-dimensional data into two or three dimensions
          - UMAP (Uniform Manifold Approximation and Projection)
            - similar to t-SNE but faster and scalable
            - preserves both local and global data structure
        - Independent Component Analysis (ICA)
          - separates a multivariate signal into additive, independent components
          - useful in signal processing and image analysis.
        - Aggregation & Combination
          - Summing or averaging features
        - for Text Data
          - Bag of Words (BoW)
            - Represents a document by counting word frequencies, ignoring word order,
            - useful for basic text classification.
          - Term Frequency-Inverse Document Frequency (TF-IDF)
            - Adjust word importance based on frequency
            - in a specific document compared to all documents
            - highlighting unique terms.
          - Word Embeddings (Word2Vec, GloVe)
        - Signal Processing Methods: used for analyzing time-series, audio and sensor data
          - Fourier Transform
            - convert signal from the time domain to the frequency domain to analyze its frequency components.
          - Wavelet Transform
            - analyzes signals that vary over time, 
            - offering both time and frequency information for non-stationary signals.
          - Short-Time Fourier Transform (STFT)
        - Image Processing Methods
          - Histogram of Oriented Gradients (HOG)
            - finds the distribution of intensity gradients or edge directions in an image
            - used in object detection and recognition tasks.
            - feature descriptor for object detection in images
            - captures edge and gradient structures.
          - Convolutional Neural Networks (CNN) Features
            - learn hierarchical features from images through layers of convolutions, 
            - ideal for classification and detection tasks.
          - Scale-Invariant Feature Transform (SIFT)
            - detects and describes local features in images
            - robust to scale, rotation, and illumination changes.
          - Speeded-Up Robust Features (SURF)
            - faster alternative to SIFT for detecting and describing image features.
      - tools for Feature Extraction
        - TensorFlow / Keras
        - PyTorch
        - NLTK(Natural Language Toolkit)
        - FeatureTools
        - Tsfresh
        - OpenCV
        - Scikit-Image
    - step includes 
      - Data Cleaning
        - handling missing values,
        - correct errors or inconsistencies
      - Data Transformation
        - raw data into a format suitable for modeling including scaling, normalization and encoding.
      - scaling numbers,
      - creating new features
      - or combining existing ones.
    - Feature Engineering Techniques
      - One-Hot Encoding
        - convert categorical variables into binary indicators
      - Binning
        - transform continuous variables into discrete bins
        - e.g labels = ['0-20', '21-40', '41-60', '61+']
      - Text Data Preprocessing
        - removing stop-words
          - filtering out commonly occurring words that
          - provide no or very little semantic value to text analysis.
            - e.g. "is", "the", "and"
          - Categories of Stopwords
            - Standard Stopwords
              - articles("a", "the"),
              - conjunctions ("and", "but")
              - and prepositions ("in", "on")
            - Domain-Specific Stopwords
            - Contextual Stopwords
              - Words with extremely high frequency
            - Numerical Stopwords
              - Digits, punctuation marks and single characters
        - Stemming
          - reducing words to their root form
          - remove prefixes and suffixes.
            - e.g. "running", "runner", and "ran" to "run"
          - Stemming Algorithms
            - Porter Stemmer
              - rule-based algorithm
              - removes common morphological and inflectional endings from words in English.
              - suffix EED to EE
                - 'agreed' → 'agree'
              - suffix ED or ING to null
                - e.g 'playing' → 'play', 'played' → 'play'
              - suffix ATION to ATE
                - e.g 'relational' → 'relate'
              - suffix TIONAL to TION
                - e.g 'conditional' → 'condition'
              - suffix FUL to null
                - e.g 'hopeful' → 'hope'
            - Snowball Stemmer(Porter2)
              - enhanced version of the Porter Stemmer
              - supports multiple languages(multilingual stemmer)
              - Example:
                - 'running' → 'run'
                - 'quickly' → 'quick'
            - Lancaster Stemmer
              - more aggressive and faster than other stemmers.
              - also more destructive and may lead to excessively shortened stems
            - Regexp Stemmer
              - Regular Expression Stemmer
              - allows users to define custom rules using regular expressions (regex).
              - 'running' → 'runn'
                Custom rule: r'ing$' removes the suffix ing.
            - Krovetz Stemmer
              - more linguistically accurate
              - preserve meaning more effectively
              - steps like converting plural forms to singular and removing ing from past-tense verbs.
              - 'children' → 'child'
        - Lemmatization
          - Reduces words to their base form (lemma) ensuring a valid word.
          - Considers the word's meaning and context to return the base form.
          - Always produces a valid word.
          - Example: "Better" → "good"
          - Considers the context and part of speech.
        - Vectorizing text data
          - transform human language into a format that machines can comprehend and process
          - numerical representations 
            - enable computers to perform tasks such as 
              - sentiment analysis, 
              - machine translation and 
              - information retrieval
          - Vectors are numerical representations of words, phrases or entire documents.
          - Techniques
            - One Hot Encoding
              - each word is represented by a vector with a high bit corresponding to 
              - the word’s index in the vocabulary 
              - all other bits set to zero.
            - Bag of Words (BoW)
              - converts text into a vector
              - representing the frequency of words, disregarding grammar and word order.
              - Ignores the order and context of words.
              - Results in high-dimensional and sparse matrices.
            - Term Frequency-Inverse Document Frequency (TF-IDF)
              - extension of BoW that weighs the frequency of words by their importance across documents.
              - reduces the weight of common words and increases the weight of rare but significant words.
              - TF(t,d)=
                Total number of terms in document d/Number of times term t appears in document d
              - IDF(t)= log_e(Total number of documents/Number of documents with term t in it)
            - Count Vectorizer
              - represents text by counting the occurrences of each word in a document.
              - similar to BoW but focuses on raw counts rather than binary presence.
              - converts a collection of text documents to a matrix of token counts
            - Word Embeddings (Word2Vec, GloVe)
              - dense vector representations of words in a continuous vector space
              - semantically similar words are located closer to each other.
            - Image Embeddings
              - transforms images into numerical representations
              - Convolutional Neural Networks (CNNs)
                - extract high-level features from images
                - represent them as dense vectors.
      - Feature Splitting
        - Divide single feature into multiple sub-features,
        - uncovering valuable insights
      - Feature Engineering tools
        - FeatureTools
        - Tsfresh
        - Pandas
        - NumPy
        - TPOT
        - DataRobot
        - Alteryx
        - H2O.ai
  - Model Selection
    - Choosing appropriate algorithms based on:
      - Problem Type
      - Data Characteristics
      - Computational Resources
  - Model Training
    - Process
      - Iterative training
      - Optimization
      - Validation
  - Model Types
    - Supervised Learning
      - Trains models on labeled data to predict or classify new, unseen data.
    - Unsupervised Learning
      - Finds patterns or groups in unlabeled data, like clustering or dimensionality reduction.
    - Reinforcement Learning
      - Learns through trial and error to maximize rewards, ideal for decision-making tasks.
    - Self-Supervised Learning
      - Utilizes large amounts of unlabeled data by 
        - generating own labels from the data itself
    - Semi-Supervised Learning
      - Combines a small amount of labeled data with a large amount of unlabeled data
  - Model Evaluation
    - Regularization Techniques: prevent overfitting by adding penalty to model complexity
      - L1 Regularization (Lasso): Least Absolute Shrinkage and Selection Operator regression
        - add the absolute value of magnitude of the coefficient as a penalty term to the loss function(L)
        - L = Loss + λ * Σ|coefficients|
        - can shrink some coefficients to zero, effectively performing feature selection.
      - L2 Regularization (Ridge)
        - add the squared magnitude of the coefficient as a penalty term to the loss function(L).
        - handle multicollinearity by shrinking the coefficients of correlated features instead of eliminating them.
        - L = Loss + λ * Σ(coefficients)²
      - Elastic Net Regression
        - combines L1 and L2 regularization
        - add the absolute norm of the weights as well as the squared measure of the weights
        - balances between feature selection and coefficient shrinkage.
        - L = Loss + λ1 * Σ|coefficients| + λ2 * Σ(coefficients)²
      - Dropout 
    - Confusion Matrix
      - True Positive
          - When the model correctly predicts the positive class.
      - True Negative
          - when the model correctly predicts the negative class.
      - False Positive
          - when the model incorrectly predicts the positive class for a negative instance.
      - False Negative
          - when the model incorrectly predicts the negative class for a positive instance.
    - Cross-Validation
      - how well model performs on unseen data while preventing overfitting
      - Train model on some parts and test it on the remaining part of dataset
      - Techniques
        - Holdout Validation
          - 50% data for training and 50% for testing.
        - LOOCV (Leave One Out Cross Validation)
          - model is trained on the entire dataset except for one data point which is used for testing.
          - repeated for each data point in the dataset.
          - computationally expensive for large datasets.
        - K-Fold Cross-Validation
          - split the dataset into k equal-sized folds.
          - train model on k-1 folds and tested on the remaining fold.
          - process is repeated k times, with each fold used as the test set once.
          - final performance is averaged over all k iterations.
          - k should be greater than or equal to 5 and less than or equal to 10.
        - Stratified K-Fold Cross-Validation
          - ensures each fold of the cross-validation process has the same class distribution as the full dataset.
          - important for imbalanced datasets
        - Leave-One-Out Cross-Validation (LOOCV)
        - Shuffle Split Cross-Validation
    - Performance Metrics
    - Metrics
    - features like:
      - Accuracy
        - ratio of correctly predicted instances to the total instances in a dataset.
        - can be misleading when one class is more dominant over the other
        - Accuracy = (TP + TN) / (TP + TN + FP + FN)
      - Precision
        - ratio of true positive predictions to the total predicted positives.
        - measures the accuracy of positive predictions.
        - important in scenarios where false positives are costly.
        - like spam detection, fraud detection etc.
        - Precision = TP / (TP + FP)
      - Recall
        - Recall (Sensitivity) is the ratio of true positive predictions to the total actual positives.
        - measures how how good the model is at predicting positives.
        - important in scenarios where false negatives are costly.
        - like disease diagnosis, safety-critical systems etc.
        - Recall = TP / (TP + FN)
      - F1-Score
        - F1-Score is the harmonic mean of Precision and Recall, 
        - providing a balance between the two metrics.
        - useful when dealing with imbalanced datasets
        - where one class is significantly more prevalent than the other.
        - F1 = 2 * (Precision * Recall) / (Precision + Recall)
      - Specificity(True Negative Rate)
        - ratio of true negative predictions to the total actual negatives.
        - measures how well the model identifies negative instances.
        - important in scenarios where false positives are costly.
        - like spam detection, fraud detection etc.
        - Specificity = TN / (TN + FP)
        - 1 - Specificity = False Positive Rate (FPR)
      - Type I Error (False Positive Rate)
        - occurs when the model incorrectly predicts the positive class for a negative instance.
        - Type I Error Rate = FP / (FP + TN)
      - Type II Error (False Negative Rate)
        - occurs when the model incorrectly predicts the negative class for a positive instance.
        - Type II Error Rate = FN / (FN + TP)
      - Mean Absolute Error (MAE)
        - average of the absolute differences between predicted and actual values.
        - provides a straightforward measure of prediction accuracy.
        - MAE = (1/n) * Σ|predicted - actual|
      - Mean Squared Error (MSE)
        - average of the squared differences between predicted and actual values.
        - penalizes larger errors more than MAE.
        - MSE = (1/n) * Σ(predicted - actual)²
      - Root Mean Squared Error (RMSE)
        - square root of the average of the squared differences between predicted and actual values.
        - provides an interpretable measure of prediction accuracy in the same units as the target variable.
        - RMSE = sqrt((1/n) * Σ(predicted - actual)²)
      - R-squared (Coefficient of Determination)
        - indicates the proportion of variance in the dependent variable that can be explained by the independent variables.
        - R² = 1 - (SS_res / SS_tot)
          - SS_res = Σ(actual - predicted)²
          - SS_tot = Σ(actual - mean(actual))²
      - ROC-AUC Curve
        - ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) 
        - Check how well a binary classification model works
        - how well the model separates the positive cases like people with a disease from the 
        - negative cases like people without the disease at different threshold level.
        - measures a model's ability to distinguish between classes, 
        - with higher values indicating better performance.
        - True Positive Rate (TPR) vs False Positive Rate (FPR)
          - True Positive Rate (TPR) = TP / (TP + FN) = Recall
          - False Positive Rate (FPR) = FP / (FP + TN) = 1 - Specificity
  - Hyperparameter Model Tuning: optimize model performance by finding the best hyperparameters
    - Grid Search CV
      - brute-force approach
      - train model using all possible combinations of specified hyperparameter values to 
      - find the best-performing setup.
      - slow and uses a lot of computer power
      - exhaustively searches through a specified subset of hyperparameters
      - evaluates model performance for each combination
      - computationally expensive for large hyperparameter spaces
      - example:
        - Tuning Logistic Regression with GridSearchCV
    - Random Search CV
      - randomly samples hyperparameter combinations from a given range
      - evaluates model performance for each combination
      - more efficient than Grid Search for large hyperparameter spaces
      - Example:
        - Tuning Random Forest with RandomizedSearchCV
        - Tuning Decision Tree with RandomizedSearchCV
    - Bayesian Optimization
      - treats hyperparameter tuning like a mathematical optimization problem and 
      - learns from past results to decide what to try next.
      - builds a probabilistic model(surrogate function) of the objective function
      - uses it to select the most promising hyperparameter combinations to evaluate
      - balances exploration and exploitation
      - P(score(y)∣hyperparameters(x))
      - surrogate function models the relationship between hyperparameters x and the score y.
      - surrogate models used in Bayesian optimization include:
        - Gaussian Processes
        - Random Forests
        - Tree-structured Parzen Estimators (TPE)
    - Genetic Algorithms
      - inspired by the process of natural selection
      - iteratively evolves a population of hyperparameter combinations
      - selects the best-performing combinations for the next generation
  - Model Deployment
    - Containerization
      - Docker
      - Kubernetes
    - Cloud Services
      - AWS SageMaker
      - Google AI Platform
      - Microsoft Azure ML
  - Model Monitoring and Maintenance
    - Model Drift Detection
    - Performance Monitoring
    - Monitoring issues like:
      - Data Drift
        - Data Drift occurs when the 
        - statistical properties of the input data change over time, 
        - leading to a decline in model performance.
      - Concept Drift
        - Concept Drift refers to changes in the 
        - underlying relationships between input features and target variables over time,
        - which can result in decreased model accuracy.
  - Algorithms
    - Regression
      - Linear Regression
        - assumes that there is a linear relationship between the input and output
        - relationship represented by a straight line.
        - uses the equation of a line: Y = mX + b
          - Y = predicted value (output)
          - m = slope of the line (coefficient)
          - X = input feature
          - b = y-intercept (constant term)
        - Simple Linear Regression
          - involves a single independent variable to predict a dependent variable.
          - equation: Y = β0 + β1X1
            - Y = predicted value
            - β0 = intercept
            - β1 = coefficient for the feature
            - X1 = input feature
        - Multiple Linear Regression
          - involves two or more independent variables to predict a dependent variable.
          - equation: Y = β0 + β1X1 + β2X2 + ... + βnXn
            - Y = predicted value
            - β0 = intercept
            - β1, β2, ..., βn = coefficients for each feature
            - X1, X2, ..., Xn = input features
          - Multicollinearity
            - occurs when two or more independent variables in a regression model are highly correlated.
            - can lead to unstable coefficient estimates and make it difficult to assess the individual effect of each predictor.
            - Detection Methods
              - Variance Inflation Factor (VIF)
                - quantifies how much the variance of a regression coefficient is increased due to multicollinearity.
                - VIF = 1 / (1 - R²)
                  - R² = coefficient of determination from regressing the predictor against all other predictors.
                - A VIF value greater than 5 or 10 indicates high multicollinearity.
              - Correlation Matrix
                - examines pairwise correlations between independent variables.
                - high correlation coefficients (e.g., above 0.8 or below -0.8) suggest multicollinearity.
              - Condition Index
                - assesses the sensitivity of the regression estimates to small changes in the data.
                - values above 30 indicate potential multicollinearity issues.
            - Remedies
              - Remove one of the correlated variables
              - Combine correlated variables into a single predictor using techniques like Principal Component Analysis (PCA)
              - Regularization methods like Ridge Regression or Lasso Regression
        - Polynomial Regression
          - models the relationship between the independent variable and dependent variable as an nth degree polynomial.
          - captures non-linear relationships by introducing polynomial terms of the independent variable.
        - use cases
          - House prices forcasting based on features like size, location, and number of bedrooms.
          - Sales forcasting based on advertising spend and market trends.
          - Estimating the impact of temperature on electricity consumption.
          - Stock price prediction based on historical data and market indicators.
          - Medical risk prediction based on patient health metrics.
        - Minimizing the error
          - Ordinary Least Squares (OLS)
            - minimizes the sum of squared differences between observed and predicted values.
            - Residuals = Observed - Predicted
            - Cost Function (SSE) = Σ(Residuals)²
          - Hypothesis in Linear Regression
            - represents the predicted output based on input features and model parameters.
            - Single linear Regression Hypothesis:
              - h(X) = β0 + β1X1
                - h(X) = predicted value
                - β0 = intercept
                - β1 = coefficient for the feature
                - X1 = input feature
            - Multiple linear Regression Hypothesis:
              - h(X) = β0 + β1X1 + β2X2 + ... + βnXn
                - h(X) = predicted value
                - β0 = intercept
                - β1, β2, ..., βn = coefficients for each feature
                - X1, X2, ..., Xn = input features
        - Cost Function
          - Mean Squared Error (MSE)
            - MSE = (1/n) * Σ(predicted - actual)²
          - Root Mean Squared Error (RMSE)
            - RMSE = sqrt((1/n) * Σ(predicted - actual)²)
          - Mean Absolute Error (MAE)
            - MAE = (1/n) * Σ|predicted - actual|
        - Gradient Descent
            - find best fit line for the data
            - iterative optimization algorithm
            - minimizes the cost function by updating model parameters in the direction of the steepest descent.
            - minimize the prediction error 
            - start with random model parameters and
            - repeatedly adjust them to reduce the difference between predicted and actual values.
            - cost function = MSE = (1/n) * Σ(predicted - actual)²
            - Gradient computation:
              - for MSE cost function, the gradient with respect to βj is:
                - (∂/∂βj) * MSE = (2/n) * Σ(predicted - actual) * Xj
                  - Xj = input feature corresponding to βj
              - update rule:
                  - βj = βj - α * (∂/∂βj) * Cost Function
                      - βj = model parameter (coefficient)
                      - α = learning rate (step size)
                      - (∂/∂βj) * Cost Function = gradient of the cost function with respect to βj
        - Gradient Descent Variants for Linear Regression
          - Batch Gradient Descent
          - Stochastic Gradient Descent (SGD)
          - Mini-Batch Gradient Descent
        - Evaluation Metrics for Linear Regression
          - R-squared (Coefficient of Determination)
            - indicates how much variation the developed model can explain or capture
            - R² = 1 - (SS_res / SS_tot)
              - SS_res = Σ(actual - predicted)²
              - SS_tot = Σ(actual - mean(actual))²
          - Residual Standard Error (RSE)
            - measures the average amount that the observed values deviate from the predicted values.
            - RSE = sqrt(SS_res / (n - p - 1))
              - SS_res = Σ(actual - predicted)²
              - n = number of observations
              - p = number of predictors
          - Residual sum of squares (RSS)
            - RSS = Σ(actual - predicted)²
          - Total sum of squares (TSS)
            - TSS = Σ(actual - mean(actual))²
          - Adjusted R-squared Error
            - Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - p - 1)]
              - n = number of observations
              - p = number of predictors
          - Mean Absolute Error (MAE)
            - MAE = (1/n) * Σ|predicted - actual|
          - Mean Squared Error (MSE)
            - average of the squared differences between the actual and predicted values for all the data points
            - gives higher weight to larger errors.
            - MSE = (1/n) * Σ(predicted - actual)²
          - Root Mean Squared Error (RMSE)
            - RMSE = sqrt((1/n) * Σ(predicted - actual)²)
      - Logistic Regression
        - Used when the output is a "yes or no" type answer
        - helps in predicting categories like pass/fail or spam/not spam.
        - predicts binary outcomes (0 or 1) based on input features.
        - used for classification problems
        - uses the logistic function (sigmoid function) to model the probability of the positive class.
        - Logistic Function (Sigmoid Function)
          - S curve
          - S(t) = 1 / (1 + e^(-t))
            - S(t) = predicted probability of the positive class
            - e = Euler's number (approximately 2.71828)
            - t = linear combination of input features and model parameters
          - Likelihood function for Logistic Regression
            - L(β) = Π P(y_i | X_i; β)
              - P(y_i | X_i; β) = S(X_i * β) if y_i = 1
              - P(y_i | X_i; β) = 1 - S(X_i * β) if y_i = 0
              - y_i = actual label (0 or 1)
              - X_i = input features for the i-th instance
              - β = model parameters (coefficients)
          - Log likelihood function
            - LL(β) = Σ [y_i * log(S(X_i * β)) + (1 - y_i) * log(1 - S(X_i * β))]
          - Gradient of the Log Likelihood
            - ∂LL(β) / ∂β = Σ (y_i - S(X_i * β)) * X_i
        - Odds and Log-Odds
          - Odds = P / (1 - P)
            - P = probability of the positive class
          - Log-Odds (Logit) = log(P / (1 - P))
        - Cost Function
          - Logistic Regression uses the Log Loss (Cross-Entropy Loss) as its cost function.
          - Log Loss(cost) = -[y * log(p) + (1 - y) * log(1 - p)]
            - y = actual label (0 or 1)
            - p = predicted probability of the positive class
        - Model Training
          - Maximum Likelihood Estimation (MLE)
            - finds the model parameters that maximize the likelihood of observing the given data.
        - Types of Logistic Regression
          - Binary Logistic Regression
            - Two classes (0 and 1)
            - e.g. spam detection, disease diagnosis etc.
          - Multinomial Logistic Regression
            - more than two classes without any order
            - e.g. classifying types of fruits (apple, banana, orange)
          - Ordinal Logistic Regression
            - more than two classes with a specific order
            - e.g. rating scales (poor, average, good, excellent)
      - Polynomial Regression
    - Decision Trees
      - supervised learning algorithm used for both classification and regression tasks
      - tree-like model of decisions and their possible consequences.
      - splits data into subsets based on feature values
      - recursively partitions the data to create a tree structure.
      - like a flowchart to help make decisions based on input features.
      - Components
        - Root Node
        - branches: attribute values/outcomes of a test
        - Internal Nodes: attribute tests
        - Leaf Nodes: Final decision o predictions(class labels or continuous values)
      - Applications
        - Customer Segmentation
        - Fraud Detection
        - Medical Diagnosis
        - Loan Approval
      - Splitting Criteria
        - Gini Index/Impurity
          - measures how often a randomly chosen element would be incorrectly labeled 
          - if it was randomly labeled according to the distribution of labels in the subset.
          - i.e. attribute with a lower Gini index should be preferred
          - IGini = 1 - Σ(p_i)²
            - p_i = proportion of instances belonging to class i
        - Information Gain
          - it tells how useful a question (or feature) is for splitting data into groups.
          - measures how much the uncertainty decreases after the split.
          - Information Gain = Entropy(Parent) - Weighted Average * Entropy(Children)
        - Mean Squared Error (MSE) for regression tasks
        - Gain(G, A) = Entropy(G) - Σ (|G_v| / |G|) * Entropy(G_v)
          - G = dataset before the split
          - A = feature used for splitting
          - G_v = subset of G where feature A has value v
      - Decision Tree Algorithms
        - ID3 (Iterative Dichotomiser 3)
          - greedily choosing the feature that maximizes the information gain at each node
          - uses entropy as the splitting criterion
          - Entropy measures impurity in the dataset
          - Entropy(S) = - Σ p_i * log2(p_i)
            - p_i = proportion of instances belonging to class i
          - Information Gain
            - IG(S, A) = Entropy(S) - Σ (|S_v| / |S|) * Entropy(S_v)
              - S = dataset before the split
              - A = feature used for splitting
              - S_v = subset of S where feature A has value v
          - it recursively splits the dataset using the feature with the highest information gain 
          - until all examples in a node belong to the same class or no features remain to split.
          - prone to overfitting
        - C4.5
          - modified version of information gain called the gain ratio
          - to reduce the bias towards features with many values
          - Gain Ratio = Information Gain / Split Information
            - Split Information = - Σ (|S_v| / |S|) * log2(|S_v| / |S|)
          - handles both continuous and categorical features
          - manages missing values effectively
          - prunes the tree after creation to reduce overfitting
          - struggles with large datasets and high-dimensional data and noisy datasets.
        - CART (Classification and Regression Trees)
          - for both classification and regression tasks
          - uses Gini impurity as the splitting criterion for classification tasks
          - which measures the impurity(likelihood of incorrect classification) of a dataset.
          - Gini Impurity = 1 - Σ(p_i)²
            - p_i = proportion of instances belonging to class i
          - for regression tasks, it uses Mean Squared Error (MSE) to minimize the variance within each node.
          - produces binary trees, where each internal node has exactly two children.
          - employs cost-complexity pruning to avoid overfitting
          - handles both numerical and categorical data effectively.
          - uses cost-complexity pruning after tree construction to reduce overfitting
          - builds binary trees
        - CHAID (Chi-squared Automatic Interaction Detector)
          - uses chi-square tests to determine the best splits especially for categorical variables.
          - chi-square tests
            - find relationship between two entities.
            - used to determine whether observed frequencies differ significantly from expected frequencies
            - under given hypothesis.
            - Applications
              - feature selection
              - goodness of fit testing
              - independence testing
              - A/B testing and feature evaluation
              - Feature selection in machine learning
              - Database Query Optimization
                - test if actual row counts per partition match the expected uniform distribution.
                - Uneven distribution (χ² significance) suggests a poor sharding strategy.
              - NLP
                - Evaluate word frequency distributions in texts.
          - recursively splits the data based on the feature that shows the most significant association with the target variable.
          - can create multi-way splits, allowing nodes to have more than two children.
          - handles both categorical and continuous features
          - Chi-square statistic (χ²)
            - χ² = Σ (O_i - E_i)² / E_i
              - O_i = observed frequency for category i
              - E_i = expected frequency for category i
          - Classification: 
            - assign a class label to new data points by following the tree from the root to a leaf node
            - with leaf node’s class label being assigned to data. 
          - Regression
            - predicts the target variable by averaging the values at the leaf node.
        - MARS (Multivariate Adaptive Regression Splines)
          - extension of the CART algorithm
          - uses splines to model non-linear relationships between variables
          - constructs a piecewise linear model where the relationship between the input and output variables
          - but with variable slopes at different points, known as knots.
          - automatically selects and positions these knots based on the data distribution and the need to capture non-linearities.
          - Basis Functions
            - h(x) = max(0, x - t) or h(x) = max(0, t - x)
              - t = knot location
          - Knot function
            - points where the slope of the piecewise linear function changes.
        - Conditional Inference Trees
          - uses statistical tests to choose splits based on the relationship between features and the target variable.
          - use permutation tests to select the feature that best splits the data while minimizing bias.
          - follows recursive approach
          - At each node it evaluates the statistical significance of potential splits 
          - using tests like the Chi-squared test for categorical features and 
          - the F-test for continuous features.
      - Types
        - Classification Trees
        - Regression Trees
        - Pruning Techniques
    - Ensemble Methods
      - Bagging
      - Boosting
      - Stacking
    - Support Vector Machines (SVM)
      - used for both classification and regression tasks
      - finds the optimal hyperplane that separates different classes in the feature space.
      - aims to maximize the margin between the hyperplane and the nearest data points from each class
      - Support Vectors
        - data points closest to the hyperplane
        - influence the position and orientation of the hyperplane.
      - Hyperplane
        - decision boundary that separates different classes in the feature space.
      - Margin
        - distance between the hyperplane and the nearest data points from each class.
      - Types
        - Linear SVM
        - Non-linear SVM
        - Kernel Functions
    - Neural Networks
      - Feedforward Neural Networks
      - Convolutional Neural Networks (CNN)
      - Recurrent Neural Networks (RNN)
  - Tools and Frameworks
    - Programming Languages
      - Python
      - R
      - Julia
    - Libraries
      - Scikit-learn
      - TensorFlow
      - PyTorch
    - Platforms
      - Google Colab
      - Jupyter Notebooks
      - Azure Machine Learning Studio
  - Applications/Use cases
    - Image Recognition
    - Natural Language Processing (NLP)
    - Recommendation Systems
    - Fraud Detection
    - Predictive Analytics
  - Overfitting
    - when a model learns the training data too well, 
    - capturing noise and details that do not generalize to new data.
  - underfitting
    - when a model is too simple to capture the underlying patterns in the data,
    - resulting in poor performance on both training and new data.
- Supervised Learning
  - Classification
    - goal is to predict discrete labels or categories
    - e.g. spam detection, image recognition, sentiment analysis etc.
  - Regression
    - aim to predict continuous numerical values
    - e.g. house price prediction, stock price forecasting, temperature prediction etc.
  - Algorithms
    - Linear Regression
      - predict numbers using a straight line.
      - helps find the relationship between input and output.
      - uses the equation of a line: Y = mX + b
        - Y = predicted value (output)
        - m = slope of the line (coefficient)
        - X = input feature
        - b = y-intercept (constant term)
    - Logistic Regression
    - Decision Trees
    - Random Forest
    - Support Vector Machines (SVM)
    - Neural Networks
- Unsupervised Learning
  - Clustering
  - Dimensionality Reduction
- Reinforcement Learning
  - Q-Learning
  - Deep Q-Networks (DQN)
  - Policy Gradient Methods
- Acronyms
  - ML: Machine Learning
  - AI: Artificial Intelligence
  - CNN: Convolutional Neural Networks
  - RNN: Recurrent Neural Networks
  - SVM: Support Vector Machines
  - AWS: Amazon Web Services
  - NLP: Natural Language Processing
  - EDA: Exploratory Data Analysis
  - ROC-AUC: Receiver Operating Characteristic - Area Under the Curve
  - 