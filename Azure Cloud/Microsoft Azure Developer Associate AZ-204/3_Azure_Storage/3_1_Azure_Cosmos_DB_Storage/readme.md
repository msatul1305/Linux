- Azure Cosmos DB Storage
  - Database types
    - Relational databases
      - SQL Server, MySQL, Postgres etc.
      - low scalability with high amount of data like IOT and social media
      - fixed schema
      - table based structure
      - vertical scaling
      - ACID guarantees(Atomicity, Consistency, Isolation and Durability)
      - Data normalization: no repetition of data: 1nf, 2nf 3nf, 4nf
    - NoSQL
      - distributed in nature
      - Fluid schema
      - Multiple structures
        - Key-value stores
        - graph databases
          - used in 
            - product recommendation engines
            - social network relationship mapping
        - document stores
        - wide-column(2-d key value store)
      - Horizontal scaling is possible using data partitioning
      - provides BASE(Basically Available, Soft state, Eventual consistency) semantics
        - data is distributed
        - hence, first data is distributed to current node and then to all other nodes i.e. soft state
        - soft state: we don't know if the data is most recent or waiting to be updated 
      - Non-normalized data: data is not optimized based on how its stored and to prevent repeating.
        - Data is optimized based on how we read it
  - COSMOS DB
    - globally distributed, multi-model db service
    - extremely low latency(single digit millisecond)
    - provides SLA for throughput, latency, availability, and consistency.
    - supports multi-region replication at any point
    - provides five-nines of high-availability for both reads and writes(99.999% availability)
    - Enables elastic scalability
    - pricing is based on throughput provisioned
    - supports multiple consistency options
    - Features
      - Integrated Analytics
        - Spark
      - Region Support
      - Schema-agnostic
        - property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary.
      - Automatic Indexing(built-in)
      - Supports multiple SDKs
      - Organization of data example:
        1. Cosmos DB account(SQL api)
        2. Database(App1)
           2.1. Container1
              2.1.1. Item1
              2.1.2. Item2
           2.2. Container2
        3. Database(App2)
  - Creating cosmos DB containers
    - Select appropriate API and SDK for a solution
      - Selecting API:
        - Cosmos DB can act like multiple types of databases
        - Supported APIs:
          - [SQL](sql.png)
            - Structured Query language(SQL)
            - JSON documents also supported
            - Database Entity: Database
            - Container Entity: Container
          - [Cassandra](cassandra.png) 
            - CQL(Cassandra Query Language)
            - wide-column format(2-d key-value store)
              - [cassandra2](cassandra2.png)
            - Database Entity: Keyspace
            - Container Entity: Table
          - [MongoDB](mongo.png)
            - JSON document store
            - Database Entity: Database
            - Container Entity: Collection
          - [Gremlin](gremlin.png)
            - graph db
            - relationship between data is visible
            - product recommendation engines, social networks, etc.
            - Apache Tinkerpop's Gremlin language for querying relationships
            - Database Entity: Database
            - Container Entity: Graph
          - [Azure Table](Azure_Table.png)
            - part of Azure Storage
            - querying data using OData or LINQ(Language Integrated Query) queries(C#)
            - Database Entity: Not Applicable
            - Container Entity: Table
        - Supported SDKs
          - for SQL API
            - Cosmos DB SDK
          - for MongoDB, Cassandra Gremlin
            - use current SDks for the API
          - For Azure Table API
            - use current Table Storage SDK
    - Process of creating cosmos DB container
      - Using portal
      - using CLI: [create_container.sh](create_container.sh)
    - Perform operations on data and cosmos DB containers
    - Set appropriate consistency level for operations
      - ***Data Consistency levels***
        - Distributed databases that rely on replication for high availability, low latency or both must make fundamental tradeoff between read consistency, availability, latency and throughput. 
        - i.e. when we write data to db, what value we get when we immediately do a read operation
        - Types:
          - ***Eventual***: Lower latency, Higher throughput, higher availability
            - but low consistency i.e. if data is in America, it may take some time to be replicated to Asia
            - ***No guarantee of order of data writing in each replica*** 
            - No guarantee of reading of own write consistency
          - ***Consistent prefix***:
            - updates are returned in order
          - ***Session***:
            - Guarantees that a client session will read its own writings.
          - ***Bounded Staleness***
            - guarantees that a read has max. lag (either number of versions or time elapsed) 
              - to get most recent value(fairly recent version)
          - ***Strong***: Higher latency, Lower throughput, Lower availability
            - Guarantees that we get the most recent version of data
      - Consistency levels for SQL APIs
        - Account default
        - Request-specific level
          - override default for specific operations
      - Consistency levels for other databases
        - Gremlin and Azure Table API: account default consistency level
        - Cassandra writes: account default consistency level
        - Cassandra reads: client consistency mapped to cosmos db level
        - Mongo DB: write - account default consistency level
        - Mongo DB: read - cosmos db level mapping
      - Throughput considerations
        - Both strong and bounded staleness reads will consume twice RU's than normal as it need to query 2 replicas.
    - Implement partitioning schemes and partition keys
      - Partitioning
        - Logical partition
          - set of items with same partition key.
          - each partition = max. 20GB storage
        - Physical partition
          - distributing data and throughput across physical partitions
          - Internally, one or more logical partitions are mapped to single physical partition.
          - Entirely managed by Azure Cosmos DB
          - ***Replica set***
            - A physical partition contains multiple replicas of data known as replica set.
            - makes storage durable and fault-tolerant
            - managed by cosmos db
        - Partition key
          - managed by user
          - helps in routing of data to correct partition
          - primary key + partition key is used for searches
          - key doesn't change for an item
          - should have different value for each container
          - Azure cosmos db uses hash-based partitioning to spread logical partitions across physical partitions.
          - Azure cosmos db hashes the partition key value of an item.
          - Then, it allocates the key space of partition key hashes evenly spread across physical partitions.
        - Strategy considerations for partition
          - HOT partition: throughput is evenly distributed across all physical partitions(e.g. if total= 20k RU(Request Units), each partition will have 10k RUs(Request Units))
          - Multi-item transactions require storage triggers or stored procedures
          - minimize cross partition queries for heavier workloads(large storage or large throughput)
          - decide on partition key strategy before creating container
    - Cosmos DB performance
      - Scaling in Traditional DB
        - Vertical scaling: inc. CPU, RAM, disks storage etc.
        - Horizontal scaling: inc. no.of VMs
          - using concept of read replicas
      - Scaling in cosmos DB
        - ***Request Unit(RU)***: similar to vertical scaling
          - increasing RU will inc CPU, RAM, IO etc. 
          - 1 RU = 1 kb item read operation from a cosmos DB container
          - Resources encapsulated in RU's
            - Processing power(CPU)
            - Memory
            - IOPS(Input/Output Operations Per Second)
        - Managing cosmos DB throughput
          - Provisioned throughput(old): specific amount set by you that is needed
            - for always on prod env
            - can be configured at db or container level
            - throughput is evenly distributed to partitions
            - requires 10RU's per GB of storage
            - requests will be rate limited(will not be completed once quota is over will ask to scale)
            - Scaling
              - manual scaling required by using RUs
              - Autoscaling
                - specify max. RU throughput and cosmos db will ensure data is available upto that throughput amount.
                - but min. throughput = 10% of max.
                - this is good for prod envs but for dev this much may not be used.
              - Serverless
                - pay for RU consumed and storage used
                - ideal for on and off workloads like dev servers
                - max . 5000 RU's
              - Best practices:
                - use partition strategy to evenly spread throughput on partitions
                - provision throughput at container level for predictable performance
                - use serverless account type for dev
                - Understand link between consistency types and amount of RU's consumed.
    - Manage change feed notifications and server side execution
      - Server side programming with cosmos DB
        - executing code based on data change using stored procedures and triggers.
      - Cosmos DB server-side concepts
        - Stored procedures
          - must be defined in JS
          - executes on a single partition, and has access to that partition only
          - partition key must be provided with the request to execute
          - supports a transaction model i.e. if something fails rollback comes into picture 
        - Triggers
          - must be defined in JS
          - can be executed either before(pre trigger) or after(post trigger) data is written to db
          - pre triggers can handle data transformation and validation
          - post triggers can handle data aggregation, change notifications etc.
          - not guaranteed to execute
            - error in pre or post trigger will result in data rollback
        - User defined functions(UDFs)
          - must be defined in JS
          - define custom functions that can be leveraged in a query
          - enables encapsulation of common logic in query conditions
        - Change feed: execution occurs external to db engine
          - react to data changes outside of cosmos data engine
          - enables to notify of any insert or update on data
          - deletes are not directly supported(i.e. deletes don't appear in the change feed)
            - but we can leverage soft-delete flag 
            - (e.g. setting "isDeleted" = True or setting TTL(Time to Life = low_value)) 
            - so that we get notifications on delete.(as it brings a change to the item)
          - A change will appear exactly once in change feed
          - reading data from db will consume throughput
          - partition updates will be in order, but between partitions no guarantee
          - not supported for Azure Table API
          - It is enabled by default for all azure Cosmos DB accounts
          - Approaches to using change feed
            - Using Azure functions
              - set up a trigger to be tied to cosmos db
              - has a built-in change feed processor
            - Using Change Feed Processor
              - run in any infra
              - used to provision workers to deal with changes.
              - manages the checkpoint of interactions with cosmos db so that data is not lost in the process.
      - Server-side execution environment
        - Stored procedures, triggers and User-defined functions(UDFs) are executed within the database engine
        - supported when using SQL api
        - supports Javascript
        - can be created and managed via portal and via SDK
- To create Cosmos DB trigger using Azure functions:
  - Portal -> function app -> create new function
  - add function -> Azure cosmos db trigger
- Notes
  - Containers are ***fundamental unit of scalability*** for cosmos DB
    - containers are unit of scalability for both provisioned throughput and storage
    - container is horizontally partitioned and then replicated across multiple regions.
  - Cosmos db hierarchy
    - Accounts -> Databases -> Containers -> Items
    - One account can have one or multiple databases
    - database == namespace
  - Cosmos Items
    - Document in a collection(Mongo)
    - row in table(SQL)
    - node or edge in graph(Gremlin)
- Selecting Partition key
  - partition key should align with data that is ***frequently accessed together*** in queries
  - partition key should have ***high cardinality***
    - Cardinality: no. of distinct values or unique values present in a set
    - high cardinality => column has many distinct values each value occurring infrequently
    - low cardinality => uneven distribution and uneven performance
  - Even workload distribution
  - smooth scaling and efficient management
  - if data is frequently accessed together, use partition key that groups them together
  - avoid using properties which have frequent updates
  - avoid monotonically increasing properties e.g. timestamps
  - Cost considerations: partition key must take less RU(Request Units)
  - must be flexible for future changes in data and access
